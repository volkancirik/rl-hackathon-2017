# rl-hackathon-2017

## Pre-requisites

### Resources
This repo assumes familiarity in ML in general(algorithms, tools). Our starting point was reading these well-written introductory posts on Reinforcement Learning. We highly recommend reading these posts before moving forward.

- [Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)
- [Q-Learning And Exploration](https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/)
- [Learning Reinforcement Learning](http://www.wildml.com/2016/10/learning-reinforcement-learning/)
- [Action-Value Methods and n-armed bandit problems](http://outlace.com/Reinforcement-Learning-Part-1/)
- [Demystifying Deep Reinforcement Learning](https://www.nervanasys.com/demystifying-deep-reinforcement-learning/)
- [Deep Deterministic Policy Gradients in TensorFlow](http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html)

In addition to these awesome resources, there are bunch of open-source RL implementations out there, we make use of following resources as starting points and built on top of them:
- [Actor Critic with OpenAI GymÂ¶](https://github.com/gregretkowski/notebooks/blob/master/ActorCritic-with-OpenAI-Gym.ipynb)
- [DQN with Experience Replay](https://github.com/sherjilozair/dqn)
- [Stochastic Policy Gradients on Pong](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)

### Tools
To move quickly we make use of following tools:
- [OpenAI's Gym](https://gym.openai.com/docs) for experimental environment. Before writing a single line of code, we recommend reading the documentation.
- [Keras](https://keras.io/) for quicky prototyping neural network models.
- [Jupyter Notebooks](http://jupyter.org/) for fast and interactive development.

## Algorithms
Here we briefly explain 3 RL algorithms where we implemented in this repository. You can use the type of agent un `run_environment.py`. Please take a look at corresponding implementation under `rl/agents/`. We use Pong as an example environment. But first we do preprocessing so that we can do faster experiments.

##### From RGB-Pong to Number-Pong
The OpenAI game [RGB-Pong](https://gym.openai.com/envs/Pong-v0) provides a 210x160x3 RGB images to be used as a state. We simplified this state to 6 numbers using image processing. The steps are visualized in this graphic:

![RGB-Pong image processing](screenshots/image_processing.png)

First we identify the field enclosed by the two white lines (1-3). Then we create a mask of all objects within the field by testing each pixel whether it is the background color (4). Both bars are 16x4. However, they can be partly occluded by the surrounding white lines. Therefore, we apply a erosion of 1x4 in the two columns where bars can occur to find them, we keep the center point of each bar (even if occluded; 5). After we have found the bars, we remove them and erode with 4x2 to find the remaining ball, which is never occluded (6).

The x position of the two bars are not relevant, leaving 4 numbers. In addition with the  movement vector of the ball, we reduced RGB-Pong to 6 numbers.

### Q-Learning
A function called Q does theoretically exist, it returns for the current state and action the total future reward until the terminal state is reached. So Q resembles the best possible scores achievable from now in game. Deep Q-Learning tries to converge to this theoretically function. A history of states, action, rewards, and new states are generated by an agent interacting with an environment. An existing rough approximation of Q can then be trained to approximate Q better by using the history:
`Q(state_t, action_t) = reward_t + decay * max_{a}(state_{t+1}, a)`
The current Q value (one the left) should be the currently received reward plus future reward from the state we transitioned into.

### Vanilla Policy Gradients
In any game, the rewards are generally revealed at the end of a small segment. There might have been many action(s) which lead up to this reward and these actions might not have received the reward they deserved. The basic idea of Policy Gradients(PG) is to guesstimate the reward for the actions which ultimately lead to a win/loss. Unlike Q-learning, Q values are not estimated in this case, instead the next best action is predicted using the policy gradient network. After a fixed number of gampeplays (also called episodes), the data and the corresponding rewards are used to train the policy network to perform incrementally better.

The beauty of this method is that the policy gradient network could be anything, starting from multilayer-perceptrons(MLPs) to RNNs to CNNs, as long as they are capable of predicting an action. Given that, it should be noted that training a policy gradient network could be a pain as most of the data that is generated is random. This might result in the creation of a biased mini-match and may end up harming the training process, rather than helping it. One of the examples of a PG agent being trained is shown below.

![Policy gradient Ping Pong](./screenshots/gif-pg.gif)

Please Note: In the implementation of Vanilla Policy Gradients, we have used crossentropy loss to train the MLP as a categorical classifier which is different from the standard approached of using MSE (mean square error) as the loss function.

### Actor-Critic Policy Gradients

[Actor-Critic Methods](https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node66.html) employ two learning module. First, `Actor` observes a state and decides on an action. `Critic` *criticez* a state by assigning a value to the state. `Actor` gets training signal from the difference between `Critic`'s state value and observed reward. `Critic`, on the other hand, gets error signal directly from reward. Starting from [Greg Retkowski's implementation](https://github.com/gregretkowski/notebooks/blob/master/ActorCritic-with-OpenAI-Gym.ipynb), we first did unit tests on Gym's `FrozenLake`. We integrated Actor-Critic implementation to our unified RL agent and did experiments with Pong. Below are some screenshots:

![Loser Agent](screenshots/gif-pongLoser.gif)

Unfortunately the model does not learn a good policy in couple of hourse we have. But [here's a lazy one](https://www.youtube.com/watch?v=N4u-mmF64tQ) learned by chance where it achieves perfect score without doing much.

## Creating A New Environment
If you would like to exploit the Gym for a target task in your mind, you can create a new environment. For an example, we would like to create an environment for a simplified computer vision problem.
We would like to localize a target object in a scene. Here agent can move a frame and the aim is to find the object in this scene.

![Object Localization](http://research.microsoft.com/en-us/um/people/jingdw/salientobjectdetection/Salient%20Object%20Detection_files/2_reg.jpg "Object Localization Example")

Starting from [CartPole](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) implementation, we implemented a simple environment. All you need to figure out how to draw simple objects. 
You can find [new environment code](rl/environments/object_localization.py) and an [example notebook](notebooks/new_environment.ipynb) under this repository. You place `object_localization.py` under `/gym/envs/classic_control/`. Also, you need to edit a couple of initialization files as explained [here](https://github.com/openai/gym/wiki/Environments). Here's an agent in action in this environment:


![Random Agent](screenshots/gif-random.gif)

![Agent Moving to Object](screenshots/gif-moving.gif)
